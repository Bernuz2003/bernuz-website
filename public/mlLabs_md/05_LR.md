### Argomento 5: Regolazione Logistica – Un Approccio Discriminativo alla Classificazione (Analisi Dettagliata)

Fino a questo punto, i nostri modelli (come il classificatore Gaussiano) sono stati **generativi**: hanno cercato di costruire un modello completo di come sono distribuiti i dati di ogni classe per poi, quasi come conseguenza, derivare un confine di separazione. Ora, invertiamo la rotta con la **Regolazione Logistica**, un modello **discriminativo**. Il suo unico scopo, diretto e pragmatico, è trovare la migliore superficie di separazione possibile tra le classi, senza porsi il problema di come i dati siano stati generati.

#### La Matematica: dal Punteggio alla Probabilità

Il cuore della Regolazione Logistica è l'idea di modellare direttamente la probabilità a posteriori che un campione **x** appartenga alla classe "Genuine" ($$c=1$$). Questo viene fatto calcolando un punteggio lineare, $$s = \mathbf{w}^T\mathbf{x} + b$$, e "schiacciandolo" tra 0 e 1 attraverso la **funzione sigmoide**, $$ \sigma(s) = 1 / (1 + e^{-s}) $$. Il risultato è la probabilità cercata:

$$
P(c=1|\mathbf{x}; \mathbf{w}, b) = \sigma(\mathbf{w}^T\mathbf{x} + b)
$$

L'obiettivo dell'addestramento è trovare i parametri ($$\mathbf{w}$$ e $$b$$) che rendono i dati di training il più "probabili" possibile secondo questo modello. Questo si traduce nel minimizzare una funzione di costo, nota come **Negative Log-Likelihood** o **Binary Cross-Entropy**. Per un singolo campione $$(\mathbf{x}_i, z_i)$$, dove $$z_i$$ è 0 o 1, la perdita è:

$$
\mathcal{L}(\mathbf{w}, b) = - [z_i \log(P(c=1|\mathbf{x}_i)) + (1-z_i) \log(1-P(c=1|\mathbf{x}_i))]
$$

Sommando su tutti gli $$N$$ campioni di training e aggiungendo il termine di **regolarizzazione L2** per prevenire l'overfitting, otteniamo la funzione obiettivo finale da minimizzare:

$$
J(\mathbf{w}, b) = \underbrace{\frac{\lambda}{2} ||\mathbf{w}||^2}_{\text{Termine di Regolarizzazione}} + \underbrace{\frac{1}{N} \sum_{i=1}^{N} \mathcal{L}_i(\mathbf{w}, b)}_{\text{Media della Cross-Entropy Loss}}
$$

Il parametro $$ \lambda > 0 $$ è un iperparametro che controlla l'intensità della regolarizzazione: un $$ \lambda $$ alto spinge i pesi **w** verso lo zero, risultando in un modello più semplice e meno incline a "memorizzare" il training set.

Nel codice, questa funzione obiettivo viene implementata e poi data in pasto a un ottimizzatore numerico (come L-BFGS) per trovare i valori di $$\mathbf{w}$$ e $$b$$ che la minimizzano.

```python
# Funzione che calcola il valore dell'obiettivo (J) e il suo gradiente
def logreg_obj_wrapper(v, D, L, l, pi_T=None):
    w, b = v[0:-1], v[-1]
    n = D.shape[1]
    
    # Calcolo dei punteggi per ogni campione
    s = w.T @ D + b
    
    # Applicazione della cross-entropy pesata (se pi_T è specificato)
    if pi_T is None:
        # Versione standard
        cxe = np.logaddexp(0, -s * (2 * L - 1)).mean()
    else:
        # Versione pesata per gestire classi sbilanciate
        Z1 = L == 1
        Z0 = L == 0
        cxe = (pi_T / Z1.sum()) * np.logaddexp(0, -s[Z1]).sum() + \
              ((1 - pi_T) / Z0.sum()) * np.logaddexp(0, s[Z0]).sum()
              
    # Funzione obiettivo completa: Regularizer + Loss
    return 0.5 * l * (w*w).sum() + cxe
```

#### Esperimento 1: L'impatto della Regolarizzazione su Dati Completi

Abbiamo addestrato il modello lineare sul training set completo (4000 campioni), esplorando come cambiano le performance al variare di $$ \lambda $$. I risultati sono visualizzati nei grafici "DCF vs Regularization" e "Model Calibration" per il Full Dataset.

![](/mlLabs_screens/05_LR/logistic_regression_Full Dataset.png)

*   **Potenziale vs. Realtà (Grafico a Sinistra)**: La curva blu del **minDCF**, che rappresenta il potenziale di separazione del modello, è straordinariamente piatta, con un valore ottimo di **0.3611** per $$ \lambda = 10^{-2} $$. Questo ci dice che la capacità intrinseca del classificatore di separare le due classi è robusta e poco sensibile alla regolarizzazione, finché questa non diventa estrema. Al contrario, la curva rossa dell'**actDCF** (la performance reale) mostra un andamento drammatico: è bassa per valori di $$ \lambda $$ piccoli ma esplode letteralmente per $$ \lambda > 10^{-1} $$, raggiungendo il valore massimo di 1.0 (classificatore inutile).
*   **La Perdita di Calibrazione (Grafico a Destra)**: Questo grafico mostra il gap tra actDCF e minDCF. Per $$ \lambda $$ bassi, il gap è minimo (sotto il 5%), indicando che i punteggi del modello sono ben calibrati. Man mano che $$ \lambda $$ aumenta, il gap cresce vertiginosamente. Questo significa che una regolarizzazione eccessiva non solo semplifica il modello, ma distrugge l'affidabilità dei suoi punteggi, rendendo la soglia teorica di Bayes completamente inefficace.

#### Esperimento 2: Prova di Forza su Dati Scarsi

Ripetere l'analisi su un sottoinsieme di soli 80 campioni di training ci mostra una dinamica completamente diversa, come illustrato nei grafici per il "Reduced Dataset".

![](/mlLabs_screens/05_LR/logistic_regression_Reduced Dataset _1_50_.png)

La lezione qui è netta: **con pochi dati, una forte regolarizzazione non è un'opzione, ma una necessità**. Il valore ottimale di $$ \lambda $$ schizza a **10**, un valore 1000 volte più grande di prima. Senza questa forte penalità, il modello cadrebbe in un overfitting profondo, imparando a memoria il rumore degli 80 campioni e fallendo miseramente nel generalizzare sui 2000 campioni di validazione. Questo esperimento dimostra magnificamente il ruolo di $$ \lambda $$ come meccanismo di controllo contro l'overfitting.

#### Esperimento 3: La Rivelazione – il Potere della Non Linearità Quadratica

La Regolazione Logistica standard disegna confini lineari (iperpiani). E se il confine reale fosse curvo? Per scoprirlo, abbiamo implementato un modello di **Regolazione Logistica Quadratica**. Questo viene fatto espandendo lo spazio delle feature: per ogni campione **x**, calcoliamo un nuovo vettore $$ \phi(\mathbf{x}) $$ che contiene le feature originali, i loro quadrati e tutti i prodotti incrociati.

```python
# Esempio di espansione quadratica
def feature_expansion(D):
    # D ha forma (M, N)
    expanded_features = [D]
    for i in range(D.shape[0]):
        for j in range(i, D.shape[0]):
            # Aggiunge il prodotto x_i * x_j
            expanded_features.append(vrow(D[i, :] * D[j, :]))
    return np.vstack(expanded_features)
```
Questo trasforma il nostro vettore da 6 a 27 dimensioni, consentendo al modello di apprendere confini di decisione iper-paraboloidi. I risultati sono stati sbalorditivi.

![](/mlLabs_screens/05_LR/logistic_regression_linear_vs_quadratic.png)

Il grafico "Linear vs Quadratic: minDCF" non lascia spazio a dubbi.
*   Il **minDCF crolla a 0.2436** (con $$ \lambda = 3.2 \times 10^{-2} $$), un miglioramento drastico e statisticamente molto significativo rispetto al 0.3611 del modello lineare.
*   La curva rossa (modello quadratico) si trova **sistematicamente e nettamente al di sotto** della curva blu (modello lineare) per quasi tutti i valori di $$ \lambda $$. Questo non è un miglioramento marginale, ma una prova schiacciante che il modello quadratico è intrinsecamente superiore per questo dataset.

#### Conclusioni Finali: Oltre la Linearità

Questa analisi approfondita ci ha portato a una conclusione fondamentale: per questo problema di classificazione biometrica, le relazioni tra le feature sono **intrinsecamente non lineari**. Un modello lineare, per quanto ben regolarizzato, non può catturare la vera complessità del confine di separazione.

Il **modello di Regolazione Logistica Quadratica** si è rivelato di gran lunga il più performante tra tutti quelli testati finora. Ha dimostrato la capacità di apprendere un confine di decisione più flessibile e accurato, risultando in un rischio di classificazione significativamente più basso. Questo risultato sposta l'asticella delle performance e stabilisce un nuovo e più potente benchmark per i modelli successivi.