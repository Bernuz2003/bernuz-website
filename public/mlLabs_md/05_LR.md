## Argomento 5: Regressione Logistica ‚Äì Un Approccio Discriminativo alla Classificazione (Analisi Dettagliata)

Fino a questo punto, i nostri modelli (come il classificatore Gaussiano) sono stati **generativi**: hanno cercato di costruire un modello completo di come sono distribuiti i dati di ogni classe per poi, quasi come conseguenza, derivare un confine di separazione. Ora, invertiamo completamente la rotta con la **Regressione Logistica**, un modello **discriminativo** per eccellenza. Il suo unico scopo, diretto e pragmatico, √® trovare la migliore superficie di separazione possibile tra le classi, senza porsi minimamente il problema di come i dati siano stati generati.

### Fondamenti Teorici: Dal Paradigma Generativo al Discriminativo

#### La Filosofia Discriminativa

I **modelli discriminativi** rappresentano un cambio di paradigma fondamentale nella classificazione. Mentre i modelli generativi cercano di rispondere alla domanda "*Come vengono generati i dati di ogni classe?*", i modelli discriminativi si concentrano esclusivamente su "*Qual √® il miglior confine di separazione tra le classi?*".

Questa differenza filosofica si traduce in vantaggi pratici significativi:

- **Efficienza parametrica**: Non √® necessario modellare la distribuzione completa dei dati
- **Robustezza**: Meno assunzioni sulla struttura dei dati
- **Focus sulla performance**: Ottimizzazione diretta della capacit√† di separazione

#### Dal Teorema di Bayes alla Probabilit√† Discriminativa

Partendo dal teorema di Bayes per il caso binario:

$$P(C=1|\mathbf{x}) = \frac{P(\mathbf{x}|C=1)P(C=1)}{P(\mathbf{x}|C=1)P(C=1) + P(\mathbf{x}|C=0)P(C=0)}$$

Se assumiamo distribuzioni Gaussiane per entrambe le classi e deriviamo il **Log-Likelihood Ratio (LLR)**, otteniamo:

$$\text{LLR}(\mathbf{x}) = \log\frac{P(\mathbf{x}|C=1)}{P(\mathbf{x}|C=0)} + \log\frac{P(C=1)}{P(C=0)}$$

Nel caso di covarianze condivise (Tied Gaussian), l'LLR diventa **lineare** in $\mathbf{x}$:

$$\text{LLR}(\mathbf{x}) = \mathbf{w}^T\mathbf{x} + b$$

dove $\mathbf{w} = \boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu}_1 - \boldsymbol{\mu}_0)$ e $b$ √® un termine di bias.

La **regressione logistica** inverte questo processo: invece di assumere distribuzioni Gaussiane e derivare la forma lineare, **assume direttamente** che l'LLR sia lineare e stima i parametri $\mathbf{w}$ e $b$ dai dati.

### La Matematica: dal Punteggio alla Probabilit√†

#### La Funzione Sigmoide: Ponte tra Lineare e Probabilistico

Il cuore della regressione logistica √® l'idea di modellare direttamente la probabilit√† a posteriori che un campione $\mathbf{x}$ appartenga alla classe "Genuine" ($c=1$). Questo avviene in due passi:

1. **Calcolo del punteggio lineare**: $s = \mathbf{w}^T\mathbf{x} + b$
2. **Trasformazione probabilistica**: Applicazione della **funzione sigmoide**

$$\sigma(s) = \frac{1}{1 + e^{-s}} = \frac{e^s}{1 + e^s}$$

La funzione sigmoide ha propriet√† matematiche eleganti:

- **Codominio**: $(0, 1)$ - perfetto per probabilit√†
- **Monotonia crescente**: $s$ maggiore ‚Üí probabilit√† maggiore
- **Derivata semplice**: $\sigma'(s) = \sigma(s)(1-\sigma(s))$
- **Relazione con odds**: $\sigma(s) = \frac{\text{odds}}{1 + \text{odds}}$ dove $\text{odds} = e^s$

#### Implementazione Numericamente Stabile

```python
def sigmoid(s):
    """
    Implementazione numericamente stabile della sigmoide.
    Evita overflow per valori molto negativi di s.
    """
    # Per s >= 0, usa la forma standard
    # Per s = 0, 
                    1 / (1 + np.exp(-s)),
                    np.exp(s) / (1 + np.exp(s)))

def log_sigmoid(s):
    """
    Calcola log(œÉ(s)) in modo numericamente stabile.
    Utile per evitare underflow nel calcolo della log-likelihood.
    """
    return -np.logaddexp(0, -s)

def log_sigmoid_complement(s):
    """
    Calcola log(1 - œÉ(s)) in modo numericamente stabile.
    """
    return -np.logaddexp(0, s)
```

#### Il Modello Probabilistico Completo

La probabilit√† che un campione $\mathbf{x}$ appartenga alla classe 1 √®:

$$P(c=1|\mathbf{x}; \mathbf{w}, b) = \sigma(\mathbf{w}^T\mathbf{x} + b)$$

E conseguentemente:

$$P(c=0|\mathbf{x}; \mathbf{w}, b) = 1 - \sigma(\mathbf{w}^T\mathbf{x} + b) = \sigma(-\mathbf{w}^T\mathbf{x} - b)$$

### Derivazione della Funzione di Costo

#### Maximum Likelihood Estimation

L'obiettivo dell'addestramento √® trovare i parametri $(\mathbf{w}, b)$ che **massimizzano la verosimiglianza** dei dati di training. Per un dataset $\{(\mathbf{x}_i, z_i)\}_{i=1}^N$ dove $z_i \in \{0, 1\}$, la likelihood √®:

$$\mathcal{L}(\mathbf{w}, b) = \prod_{i=1}^N P(c=z_i|\mathbf{x}_i; \mathbf{w}, b)$$

$$= \prod_{i=1}^N [\sigma(\mathbf{w}^T\mathbf{x}_i + b)]^{z_i} [1-\sigma(\mathbf{w}^T\mathbf{x}_i + b)]^{1-z_i}$$

#### Dalla Likelihood alla Cross-Entropy Loss

Passando ai logaritmi e cambiando segno per ottenere un problema di **minimizzazione**:

$$\mathcal{L}_{\text{CE}}(\mathbf{w}, b) = -\frac{1}{N}\sum_{i=1}^N [z_i \log(\sigma(s_i)) + (1-z_i) \log(1-\sigma(s_i))]$$

dove $s_i = \mathbf{w}^T\mathbf{x}_i + b$.

Questa √® la celebre **Binary Cross-Entropy Loss**, che pu√≤ essere riscritta in forma pi√π compatta usando la trasformazione $\tilde{z}_i = 2z_i - 1 \in \{-1, +1\}$:

$$\mathcal{L}_{\text{CE}}(\mathbf{w}, b) = \frac{1}{N}\sum_{i=1}^N \log(1 + e^{-\tilde{z}_i s_i})$$

#### Regolarizzazione L2: Controllo della Complessit√†

Per prevenire l'overfitting, si aggiunge un termine di **regolarizzazione L2**:

$$J(\mathbf{w}, b) = \underbrace{\frac{\lambda}{2} \|\mathbf{w}\|^2}_{\text{L2 Regularizer}} + \underbrace{\frac{1}{N} \sum_{i=1}^{N} \log(1 + e^{-\tilde{z}_i s_i})}_{\text{Cross-Entropy Loss}}$$

Il parametro $\lambda > 0$ controlla l'intensit√† della regolarizzazione:
- **$\lambda$ basso**: Modello pi√π complesso, possibile overfitting
- **$\lambda$ alto**: Modello pi√π semplice, possibile underfitting
- **$\lambda = 0$**: Nessuna regolarizzazione (pericoloso con pochi dati)

#### Implementazione Efficiente della Funzione Obiettivo

```python
def logistic_regression_objective(v, D, L, lambda_reg):
    """
    Calcola la funzione obiettivo della regressione logistica.
    
    Args:
        v: vettore dei parametri [w1, w2, ..., wM, b]
        D: matrice delle features (M, N)
        L: vettore delle label (N,) con valori {0, 1}
        lambda_reg: parametro di regolarizzazione
    
    Returns:
        J: valore della funzione obiettivo
        grad: gradiente rispetto ai parametri
    """
    # Estrai parametri
    w = v[:-1]  # Primi M elementi sono i pesi
    b = v[-1]   # Ultimo elemento √® il bias
    
    N = D.shape[1]
    
    # Calcola i punteggi
    scores = w.T @ D + b  # Shape: (N,)
    
    # Trasforma le label in {-1, +1}
    z_tilde = 2 * L - 1  # Shape: (N,)
    
    # Cross-entropy loss (numericamente stabile)
    ce_loss = np.logaddexp(0, -z_tilde * scores).mean()
    
    # Regolarizzazione L2
    l2_reg = 0.5 * lambda_reg * np.sum(w ** 2)
    
    # Funzione obiettivo totale
    J = l2_reg + ce_loss
    
    # Calcolo del gradiente
    # ‚àÇJ/‚àÇw = Œªw + (1/N) Œ£ ‚àÇCE/‚àÇs * ‚àÇs/‚àÇw
    # ‚àÇJ/‚àÇb = (1/N) Œ£ ‚àÇCE/‚àÇs * ‚àÇs/‚àÇb
    
    # Derivata della cross-entropy: ‚àÇCE/‚àÇs = -z_tilde * œÉ(-z_tilde * s)
    prob_wrong = sigmoid(-z_tilde * scores)  # P(predizione sbagliata)
    grad_ce_scores = -z_tilde * prob_wrong   # Shape: (N,)
    
    # Gradienti finali
    grad_w = lambda_reg * w + (grad_ce_scores @ D.T) / N  # Shape: (M,)
    grad_b = grad_ce_scores.mean()  # Scalare
    
    grad = np.concatenate([grad_w, [grad_b]])
    
    return J, grad
```

### Ottimizzazione: Algoritmi e Implementazione

#### L-BFGS: Quasi-Newton per Problemi Non Lineari

La regressione logistica non ha una soluzione in forma chiusa, quindi richiede **ottimizzazione iterativa**. L'algoritmo **L-BFGS** (Limited-memory Broyden-Fletcher-Goldfarb-Shanno) √® particolarmente efficace perch√©:

1. **Approssima la matrice Hessiana** senza calcolarla esplicitamente
2. **Memoria limitata**: Adatto a problemi con molte features
3. **Convergenza superlineare**: Molto pi√π veloce del gradient descent
4. **Gestione automatica dello step size**: Con line search

#### Implementazione del Training

```python
def train_logistic_regression(DTR, LTR, lambda_reg):
    """
    Addestra un modello di regressione logistica.
    
    Args:
        DTR: matrice features di training (M, N)
        LTR: vettore label di training (N,) con valori {0, 1}
        lambda_reg: parametro di regolarizzazione
    
    Returns:
        w: vettore dei pesi (M,)
        b: bias (scalare)
    """
    M = DTR.shape[0]  # Numero di features
    
    # Trasforma le label in {-1, +1} per stabilit√† numerica
    ZTR = 2 * LTR - 1
    
    def objective_function(v):
        """Wrapper per scipy.optimize"""
        return logistic_regression_objective(v, DTR, ZTR, lambda_reg)
    
    # Inizializzazione: parametri piccoli casuali
    np.random.seed(0)  # Per riproducibilit√†
    v0 = np.random.normal(0, 0.01, M + 1)  # w + b
    
    # Ottimizzazione con L-BFGS
    result = scipy.optimize.fmin_l_bfgs_b(
        func=objective_function,
        x0=v0,
        approx_grad=False,  # Usiamo il gradiente esatto
        maxiter=1000,
        factr=1e7,  # Tolleranza di convergenza
        pgtol=1e-6
    )
    
    # Estrai parametri ottimali
    v_opt = result[0]
    w = v_opt[:-1]
    b = v_opt[-1]
    
    # Diagnostiche
    if result[2]['warnflag'] != 0:
        print(f"‚ö†Ô∏è  Avviso convergenza: {result[2]['task']}")
    
    print(f"‚úÖ Convergenza raggiunta in {result[2]['nit']} iterazioni")
    print(f"üìä Valore finale della funzione obiettivo: {result[1]:.6f}")
    
    return w, b

def predict_logistic_regression(w, b, DVAL):
    """
    Calcola predizioni e probabilit√† per nuovi campioni.
    
    Args:
        w: vettore dei pesi
        b: bias
        DVAL: matrice features di validazione (M, N_val)
    
    Returns:
        scores: punteggi lineari (N_val,)
        probabilities: probabilit√† P(c=1|x) (N_val,)
        predictions: predizioni binarie {0, 1} (N_val,)
    """
    # Calcola punteggi
    scores = w.T @ DVAL + b
    
    # Converte in probabilit√†
    probabilities = sigmoid(scores)
    
    # Predizioni binarie (soglia 0.5)
    predictions = (probabilities >= 0.5).astype(int)
    
    return scores, probabilities, predictions
```

### Modelli Avanzati: Estensioni della Regressione Logistica

#### 1. Regressione Logistica Pesata: Gestione di Classi Sbilanciate

Quando le classi sono sbilanciate o quando il **prior target** differisce da quello empirico, √® utile utilizzare una versione **pesata** della regressione logistica.

##### Formulazione Matematica

La funzione obiettivo pesata √®:

$$J_w(\mathbf{w}, b) = \frac{\lambda}{2} \|\mathbf{w}\|^2 + \sum_{i=1}^N w_i \log(1 + e^{-\tilde{z}_i s_i})$$

dove i pesi $w_i$ sono definiti come:

- $w_i = \frac{\pi_T}{N_1}$ se $z_i = 1$ (classe positiva)
- $w_i = \frac{1-\pi_T}{N_0}$ se $z_i = 0$ (classe negativa)

con $\pi_T$ prior target, $N_1$ numero di campioni positivi, $N_0$ numero di campioni negativi.

##### Implementazione

```python
def train_weighted_logistic_regression(DTR, LTR, lambda_reg, target_prior):
    """
    Addestra regressione logistica con pesatura per prior diversi.
    
    Args:
        DTR: features di training (M, N)
        LTR: label di training (N,) con valori {0, 1}
        lambda_reg: parametro di regolarizzazione
        target_prior: prior target per la classe positiva
    
    Returns:
        w, b: parametri del modello
    """
    ZTR = 2 * LTR - 1  # Trasforma in {-1, +1}
    
    # Calcola i pesi per bilanciare le classi
    N_pos = np.sum(LTR == 1)  # Campioni classe 1
    N_neg = np.sum(LTR == 0)  # Campioni classe 0
    
    w_pos = target_prior / N_pos
    w_neg = (1 - target_prior) / N_neg
    
    print(f"üìä Pesi classi: Positiva = {w_pos:.4f}, Negativa = {w_neg:.4f}")
    print(f"üìä Prior empirico: {N_pos/(N_pos+N_neg):.3f} ‚Üí Prior target: {target_prior:.3f}")
    
    def weighted_objective(v):
        w = v[:-1]
        b = v[-1]
        
        scores = w.T @ DTR + b
        
        # Calcola loss pesata
        losses = np.logaddexp(0, -ZTR * scores)
        
        # Applica pesi
        weights = np.where(LTR == 1, w_pos, w_neg)
        weighted_loss = np.sum(weights * losses)
        
        # Regolarizzazione
        l2_reg = 0.5 * lambda_reg * np.sum(w ** 2)
        
        # Gradiente pesato
        prob_wrong = sigmoid(-ZTR * scores)
        grad_ce = -ZTR * prob_wrong * weights
        
        grad_w = lambda_reg * w + grad_ce @ DTR.T
        grad_b = np.sum(grad_ce)
        
        return l2_reg + weighted_loss, np.concatenate([grad_w, [grad_b]])
    
    # Ottimizzazione
    v0 = np.zeros(DTR.shape[0] + 1)
    result = scipy.optimize.fmin_l_bfgs_b(weighted_objective, v0)
    
    return result[0][:-1], result[0][-1]
```

#### 2. Regressione Logistica Quadratica: Catturare Non-Linearit√†

Per catturare relazioni non-lineari, si pu√≤ espandere lo spazio delle features includendo **termini quadratici** e **prodotti incrociati**.

##### Espansione delle Features

Per un vettore $\mathbf{x} = [x_1, x_2, \ldots, x_M]^T$, l'espansione quadratica include:

1. **Features originali**: $x_1, x_2, \ldots, x_M$
2. **Termini quadratici**: $x_1^2, x_2^2, \ldots, x_M^2$
3. **Prodotti incrociati**: $x_i x_j$ per $i  10:
        print(f"  ... e altre {len(feature_names)-10} features")
    
    return D_expanded, feature_names

def analyze_feature_importance(w, feature_names, top_k=10):
    """
    Analizza l'importanza delle features basata sui pesi del modello.
    """
    importance = np.abs(w)
    sorted_indices = np.argsort(importance)[::-1]
    
    print(f"\nüìä TOP {top_k} FEATURES PI√ô IMPORTANTI:")
    print("-" * 50)
    for i in range(min(top_k, len(w))):
        idx = sorted_indices[i]
        print(f"{i+1:2d}. {feature_names[idx]: 10^{-2}$
- Esplosione a 1.0 (performance casuale) per $\lambda \geq 1$
- Indica **perdita progressiva di calibrazione**

**Gap di Calibrazione**:
La differenza $\text{actDCF} - \text{minDCF}$ quantifica quanto male sono calibrati i punteggi del modello:

```python
def analyze_calibration_quality(minDCF, actDCF):
    """Analizza la qualit√† della calibrazione"""
    gap = actDCF - minDCF
    
    if gap  z_critical, z_score

significant, z_score = statistical_significance_test(0.3611, 0.2436)
print(f"Z-score: {z_score:.2f}")
print(f"Significativo: {significant}")
# Output: Z-score: 10.87, Significativo: True
```

##### Interpretazione delle Features Quadratiche

Le features pi√π importanti nel modello quadratico rivelano le strutture nascoste nei dati:

```python
# Esempio di analisi dei pesi (simulato dai risultati)
top_features_quadratic = [
    ("x_3^2", -0.2847),      # Forte effetto quadratico sulla feature 3
    ("x_4^2", +0.2334),      # Forte effetto quadratico sulla feature 4  
    ("x_3*x_4", -0.1892),    # Interazione significativa tra feature 3 e 4
    ("x_1*x_2", +0.1567),    # Interazione tra feature 1 e 2
    ("x_3", -0.1234),        # Effetto lineare residuo della feature 3
    ("x_4", +0.1156),        # Effetto lineare residuo della feature 4
]
```

Questo pattern conferma che:
1. **Features 3 e 4** sono centrali (come visto negli esperimenti precedenti)
2. **Gli effetti quadratici** $x_3^2$ e $x_4^2$ sono cruciali
3. **L'interazione** $x_3 \cdot x_4$ cattura dipendenze non lineari
4. Il confine ottimale √® una **superficie quadratica**, non un iperpiano

### Confronto Finale e Interpretazione Avanzata

#### Ranking Completo dei Modelli

| Ranking | Modello | minDCF | actDCF | Œª ottimo | Calibrazione | Note |
|---------|---------|---------|---------|----------|--------------|------|
| ü•á **1¬∞** | **Quadratic LR** | **0.2436** | 0.4952 | 3.2e-02 | Poor | **Rivoluzionario** |
| ü•à 2¬∞ | Linear LR (Full) | 0.3611 | 0.4568 | 1.0e-02 | Poor | Baseline solida |
| ü•â 3¬∞ | Weighted LR | 0.3620 | 1.0000 | 3.2e+01 | Very Poor | Beneficio marginale |
| 4¬∞ | Linear LR (Reduced) | 0.3783 | 1.0000 | 1.0e+01 | Very Poor | Limite dati scarsi |

#### Analisi del Bias-Variance Trade-off

```python
def analyze_bias_variance_tradeoff():
    """
    Analisi teorica del trade-off per i diversi modelli
    """
    models = {
        "Linear LR": {
            "parameters": 7,  # 6 weights + 1 bias
            "capacity": "Medium",
            "bias": "Medium",
            "variance": "Medium"
        },
        "Quadratic LR": {
            "parameters": 28,  # 27 weights + 1 bias  
            "capacity": "High",
            "bias": "Low",
            "variance": "High"
        },
        "Weighted LR": {
            "parameters": 7,
            "capacity": "Medium",
            "bias": "Medium-High",  # Bias verso prior target
            "variance": "Medium"
        }
    }
    
    return models
```

La **superiorit√† del modello quadratico** indica che:
1. Il **bias** del modello lineare √® il fattore limitante
2. Con 4000 campioni, c'√® capacit√† sufficiente per stimare 28 parametri
3. La **varianza aggiuntiva** √® pi√π che compensata dalla **riduzione del bias**

#### Implicazioni per il Machine Learning Applicato

##### 1. Feature Engineering vs. Model Complexity

Il successo della regressione logistica quadratica suggerisce che:
- **L'espansione delle features** pu√≤ essere pi√π efficace di modelli pi√π complessi
- **Le interazioni tra features** sono spesso cruciali
- **La regolarizzazione** diventa ancora pi√π importante con features espanse

##### 2. Calibrazione vs. Discriminazione

Tutti i modelli mostrano **calibrazione scarsa** nonostante **buona discriminazione**:
- **minDCF basso**: Il modello pu√≤ separare bene le classi
- **actDCF alto**: I punteggi numerici non riflettono probabilit√† reali

Questo suggerisce la necessit√† di **post-processing di calibrazione**:

```python
def platt_scaling_calibration(scores_val, labels_val):
    """
    Calibrazione di Platt: applica regressione logistica ai punteggi
    """
    # Fit di una sigmoide ai punteggi: P_cal = œÉ(A*score + B)
    def objective(params):
        A, B = params
        probs = sigmoid(A * scores_val + B)
        # Cross-entropy loss
        return -np.mean(labels_val * np.log(probs + 1e-15) + 
                       (1 - labels_val) * np.log(1 - probs + 1e-15))
    
    result = scipy.optimize.minimize(objective, [1.0, 0.0])
    A_opt, B_opt = result.x
    
    return A_opt, B_opt

def isotonic_regression_calibration(scores_val, labels_val):
    """
    Calibrazione isotonica: mappatura monotona non-parametrica
    """
    from sklearn.isotonic import IsotonicRegression
    
    iso_reg = IsotonicRegression(out_of_bounds='clip')
    calibrated_probs = iso_reg.fit_transform(scores_val, labels_val)
    
    return iso_reg
```

##### 3. Regolarizzazione Adattiva

I risultati mostrano che il **Œª ottimo dipende fortemente dalla dimensione del dataset**:

```python
def adaptive_regularization_strategy(n_samples, n_features):
    """
    Strategia adattiva per la scelta di Œª
    """
    # Rapporto campioni/parametri
    ratio = n_samples / n_features
    
    if ratio > 100:
        # Molti dati: regolarizzazione leggera
        lambda_range = np.logspace(-4, -1, 10)
    elif ratio > 50:
        # Dati moderati: regolarizzazione media
        lambda_range = np.logspace(-3, 0, 10)
    elif ratio > 10:
        # Pochi dati: regolarizzazione forte
        lambda_range = np.logspace(-2, 1, 10)
    else:
        # Dati molto scarsi: regolarizzazione molto forte
        lambda_range = np.logspace(-1, 2, 10)
    
    return lambda_range

# Per il nostro caso:
# Dataset completo: 4000/6 = 667 ‚Üí regolarizzazione leggera ‚úì
# Dataset ridotto: 80/6 = 13 ‚Üí regolarizzazione forte ‚úì
```

### Implementazione Completa e Produzione

#### Framework Unificato per Regressione Logistica

```python
class LogisticRegressionClassifier:
    """
    Implementazione completa della regressione logistica con tutte le varianti.
    """
    
    def __init__(self, regularization='l2', lambda_reg=1e-3, 
                 prior_weighted=False, target_prior=0.5,
                 feature_expansion='linear'):
        """
        Args:
            regularization: 'l2', 'l1', o 'elastic_net'
            lambda_reg: parametro di regolarizzazione
            prior_weighted: se usare pesatura per prior diversi
            target_prior: prior target (se prior_weighted=True)
            feature_expansion: 'linear', 'quadratic', 'polynomial'
        """
        self.regularization = regularization
        self.lambda_reg = lambda_reg
        self.prior_weighted = prior_weighted
        self.target_prior = target_prior
        self.feature_expansion = feature_expansion
        
        # Parametri del modello (impostati durante training)
        self.w = None
        self.b = None
        self.feature_names = None
        self.training_stats = {}
        
    def _expand_features(self, D):
        """Espansione delle features secondo la strategia scelta"""
        if self.feature_expansion == 'linear':
            return D, [f"x_{i+1}" for i in range(D.shape[0])]
        elif self.feature_expansion == 'quadratic':
            return expand_features_quadratic(D)
        else:
            raise ValueError(f"Espansione non supportata: {self.feature_expansion}")
    
    def fit(self, D_train, L_train, verbose=True):
        """
        Addestramento del modello
        """
        if verbose:
            print(f"üöÄ Training Logistic Regression:")
            print(f"   Regularization: {self.regularization}")
            print(f"   Lambda: {self.lambda_reg}")
            print(f"   Feature expansion: {self.feature_expansion}")
            print(f"   Prior weighting: {self.prior_weighted}")
        
        # Espansione features
        D_expanded, self.feature_names = self._expand_features(D_train)
        
        # Training con parametri specificati
        if self.prior_weighted:
            self.w, self.b = train_weighted_logistic_regression(
                D_expanded, L_train, self.lambda_reg, self.target_prior
            )
        else:
            self.w, self.b = train_logistic_regression(
                D_expanded, L_train, self.lambda_reg
            )
        
        # Statistiche di training
        self.training_stats = {
            'n_samples': D_train.shape[1],
            'n_features_original': D_train.shape[0],
            'n_features_expanded': D_expanded.shape[0],
            'expansion_factor': D_expanded.shape[0] / D_train.shape[0],
            'regularization_strength': self.lambda_reg
        }
        
        if verbose:
            print(f"‚úÖ Training completato!")
            print(f"   Features: {self.training_stats['n_features_original']} ‚Üí {self.training_stats['n_features_expanded']}")
            print(f"   Campioni: {self.training_stats['n_samples']}")
        
        return self
    
    def predict_proba(self, D_test):
        """Predizione di probabilit√†"""
        if self.w is None:
            raise ValueError("Modello non addestrato. Chiama fit() prima.")
        
        D_expanded, _ = self._expand_features(D_test)
        scores = self.w.T @ D_expanded + self.b
        probabilities = sigmoid(scores)
        
        return np.column_stack([1 - probabilities, probabilities])
    
    def predict(self, D_test, threshold=0.5):
        """Predizione di classe"""
        probs = self.predict_proba(D_test)
        return (probs[:, 1] >= threshold).astype(int)
    
    def decision_function(self, D_test):
        """Punteggi di decisione (prima della sigmoide)"""
        if self.w is None:
            raise ValueError("Modello non addestrato.")
        
        D_expanded, _ = self._expand_features(D_test)
        return self.w.T @ D_expanded + self.b
    
    def get_feature_importance(self, top_k=10):
        """Analisi dell'importanza delle features"""
        if self.w is None:
            raise ValueError("Modello non addestrato.")
        
        importance = np.abs(self.w)
        sorted_indices = np.argsort(importance)[::-1]
        
        results = []
        for i in range(min(top_k, len(self.w))):
            idx = sorted_indices[i]
            results.append({
                'feature': self.feature_names[idx],
                'weight': self.w[idx],
                'abs_weight': importance[idx],
                'rank': i + 1
            })
        
        return results
```

### Conclusioni e Direzioni Future

#### Risultati Principali

L'analisi approfondita della regressione logistica ha rivelato insights fondamentali:

1. **La non-linearit√† √® cruciale**: Il modello quadratico migliora le performance del 32.5%
2. **La regolarizzazione √® dataset-dipendente**: Œª ottimo varia di 1000x tra dataset completo e ridotto  
3. **La calibrazione √® problematica**: Tutti i modelli mostrano gap significativi tra minDCF e actDCF
4. **Le interazioni contano**: Features 3 e 4 e la loro interazione sono discriminanti chiave

#### Implicazioni Teoriche

- **Bias-Variance Trade-off**: Con dati sufficienti, ridurre il bias (modello quadratico) √® pi√π importante che controllare la varianza
- **Feature Engineering**: L'espansione manuale delle features pu√≤ essere pi√π efficace di modelli intrinsecamente non-lineari
- **Regolarizzazione Adattiva**: La strategia di regolarizzazione deve adattarsi alla dimensione del dataset

#### Direzioni Future

1. **Calibrazione Avanzata**: Implementare Platt scaling e isotonic regression
2. **Feature Selection**: Identificare automaticamente le interazioni pi√π importanti
3. **Regolarizzazione Adattiva**: Sviluppare strategie data-driven per la scelta di Œª
4. **Ensemble Methods**: Combinare modelli lineari e quadratici per robustezza
5. **Analisi di Sensibilit√†**: Studio della stabilit√† dei risultati rispetto ai parametri

La regressione logistica, apparentemente semplice, si rivela un framework ricco e potente che, con le giuste estensioni, pu√≤ competere con modelli molto pi√π complessi, offrendo il vantaggio aggiuntivo dell'interpretabilit√† e dell'efficienza computazionale.