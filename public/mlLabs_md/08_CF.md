### Argomento 8: Calibrazione e Fusione – L'Atto Finale di Ottimizzazione

Nel nostro percorso attraverso il machine learning, abbiamo costruito un arsenale di classificatori, dai modelli generativi (GMM) ai discriminativi (Regolazione Logistica, SVM). Abbiamo visto che alcuni modelli, come le SVM con kernel RBF e i GMM, offrono un'eccellente capacità di separazione (basso `minDCF`), ma altri, in particolare le SVM, producono punteggi grezzi che sono poco affidabili e "mal calibrati". Questo capitolo finale è dedicato a due tecniche avanzate per affrontare questi problemi:
1.  **Calibrazione**: Un processo per trasformare i punteggi grezzi di un classificatore in probabilità a posteriori ben formate, con l'obiettivo di rendere il rischio di Bayes effettivo (`actDCF`) il più vicino possibile al rischio minimo potenziale (`minDCF`).
2.  **Fusione**: Una tecnica per combinare le predizioni di più classificatori in un unico "super-modello", sperando di superare le performance di ogni singolo componente.

L'analisi si svolgerà su una nuova suddivisione dei dati: un set di training (3200 campioni), un set di validazione (1600 campioni) su cui verranno ottimizzate le strategie di calibrazione e fusione, e un set di valutazione finale (1200 campioni), tenuto completamente da parte per la valutazione finale e imparziale del sistema migliore.

#### Parte 1: Calibrazione – Insegnare ai Modelli a Parlare il Linguaggio della Probabilità

Un classificatore può essere molto bravo a separare le classi (basso `minDCF`) ma pessimo nel quantificare la sua fiducia (alto `actDCF`). Questo accade quando i suoi punteggi non si comportano come Log-Likelihood Ratios. La calibrazione risolve questo problema addestrando un secondo modello, semplice, che impara a "correggere" i punteggi del primo.

##### La Matematica della Calibrazione

La calibrazione viene implementata come una semplice trasformazione affine dei punteggi originali, seguita da una sigmoide. In pratica, si addestra un modello di **Regolazione Logistica** utilizzando i punteggi del classificatore originale $$s_{\text{orig}}$$ come unica feature:

$$
s_{\text{calibrato}} = \alpha s_{\text{orig}} + \beta
$$

I parametri di calibrazione, $$ \alpha $$ (pendenza) e $$ \beta $$ (intercetta), vengono appresi minimizzando la solita funzione di costo della cross-entropy su un set di dati di calibrazione. Un aspetto cruciale è la scelta del prior $$ p_T $$ utilizzato per addestrare questo modello di calibrazione. Abbiamo usato una validazione incrociata a 5 fold (K-fold) per trovare il valore di $$ p_T $$ ottimale per ogni modello.

```python
def kfold_calibration_analysis(scores, labels, target_prior=0.1, KFOLD=5):
    # ...
    for pT in training_priors: # Testa diversi pT
        calibrated_scores = []
        # K-fold cross-validation
        for foldIdx in range(KFOLD):
            # Suddivide i dati in fold di calibrazione e validazione
            SCAL, SVAL = extract_train_val_folds_from_ary(scores, foldIdx, KFOLD)
            LCAL, LVAL_fold = extract_train_val_folds_from_ary(labels, foldIdx, KFOLD)
            
            # Addestra il modello di calibrazione (Regolazione Logistica)
            w, b = logReg.trainWeightedLogRegBinary(vrow(SCAL), LCAL, 0, pT)
            
            # Applica la calibrazione e calcola i nuovi punteggi
            calibrated_SVAL = (w.T @ vrow(SVAL) + b - np.log(pT / (1-pT))).ravel()
            calibrated_scores.append(calibrated_SVAL)
        # ...
    # Trova il miglior pT basato sull'actDCF medio
    return results, best_pT
```

##### Analisi dei Risultati di Calibrazione

Abbiamo applicato questo processo ai nostri quattro migliori modelli, con risultati molto diversi che rivelano la natura di ciascun classificatore.

| Modello                 | `actDCF` Originale | `actDCF` Calibrato | Miglioramento |
| :---------------------- | :----------------- | :----------------- | :------------ |
| **SVM (RBF)**           | 1.0000             | 0.3111             | **+68.9%**    |
| **Regolazione Logistica** | 0.4823             | 0.4703             | +2.5%         |
| **GMM Diagonale**       | 0.1899             | 0.1873             | +1.4%         |
| **GMM Completa**        | 0.1899             | 0.2060             | -8.5%         |

*   **SVM: La Trasformazione Radicale**: La SVM, originariamente la peggiore in termini di `actDCF` (1.0000, completamente inutile), ha visto un miglioramento spettacolare del 68.9%. Questo conferma che la SVM è un potente separatore, ma i suoi punteggi basati sul margine non hanno un'interpretazione probabilistica. La calibrazione è **assolutamente essenziale** per renderla utilizzabile. L'effetto è visibile nel grafico "Calibration Comparison SVM", dove la curva dell'actDCF (linea continua) passa dall'essere una "V" stretta e inutile a una curva rossa che segue molto da vicino il potenziale del modello (linea tratteggiata).

*   **GMM: Già Calibrati per Natura**: I modelli GMM, essendo generativi e basati su probabilità, erano già ben calibrati. Il processo di calibrazione ha portato solo a un piccolo miglioramento per il modello diagonale e addirittura a un leggero peggioramento per quello completo. Questo non è sorprendente: tentare di calibrare un modello già buono con dati limitati (i fold della cross-validation) può introdurre piccoli errori di stima.
*   **Regolazione Logistica**: Anche questo modello ha visto un piccolo beneficio, dimostrando che, sebbene sia intrinsecamente probabilistico, una fase di "fine-tuning" dei punteggi può comunque limare le imperfezioni.

#### Parte 2: Fusione – L'Unione Fa la Forza

La fusione parte dall'idea che modelli diversi possano commettere errori diversi. Combinando i loro output, possiamo sperare di creare un classificatore più robusto e accurato del miglior singolo componente.

##### La Matematica della Fusione

La nostra strategia di fusione è, ancora una volta, basata sulla Regolazione Logistica. Trattiamo i punteggi dei nostri quattro sistemi (GMM Completo, GMM Diagonale, Regolazione Logistica, SVM) come un vettore di feature a 4 dimensioni. Il modello di fusione impara una combinazione lineare pesata di questi punteggi:

$$
s_{\text{fuso}} = \mathbf{w}^T \mathbf{s}_{\text{vettore}} + b
$$

Dove $$ \mathbf{s}_{\text{vettore}} = [s_{\text{GMM-F}}, s_{\text{GMM-D}}, s_{\text{LR}}, s_{\text{SVM}}]^T $$. I pesi **w** vengono appresi per minimizzare la cross-entropy sul set di validazione. Questo permette al modello di fusione di dare più importanza ai classificatori più affidabili e performanti.

```python
def kfold_fusion_analysis(scores_dict, labels, target_prior=0.1, KFOLD=5):
    # Concatena i punteggi dei diversi modelli in una matrice di feature
    score_arrays = [scores_dict[name]['scores'] for name in score_names]
    # ...
    for foldIdx in range(KFOLD):
        # ...
        # Crea la matrice di feature per il training della fusione
        SCAL_matrix = np.vstack(SCAL_arrays)
        SVAL_matrix = np.vstack(SVAL_arrays)
        
        # Addestra il modello di fusione (Regolazione Logistica)
        w, b = logReg.trainWeightedLogRegBinary(SCAL_matrix, LCAL, 0, pT)
        
        # Calcola i punteggi fusi
        fused_SVAL = (w.T @ SVAL_matrix + b - np.log(pT / (1-pT))).ravel()
        # ...
    return results, best_pT
```

##### Analisi dei Risultati di Fusione

Dopo aver trovato il prior di training ottimale ($$ p_T = 0.1 $$), abbiamo confrontato il modello di fusione con il miglior sistema individuale (il GMM Diagonale calibrato).

*   Miglior Sistema Individuale (`GMM Diagonale`): `actDCF` = 0.1873
*   **Sistema di Fusione**: `actDCF` = **0.1785**

La fusione ha portato a un ulteriore miglioramento del 4.7%. Sebbene possa sembrare un piccolo passo, nel mondo dei sistemi ad alte prestazioni ogni frazione di punto percentuale è una vittoria significativa. Il grafico "Fusion Comparison" mostra visivamente questo successo: la curva del DCF del modello fuso (linea rossa) si posiziona leggermente al di sotto di quella del miglior modello individuale (linea blu punteggiata).

#### Parte 3: Il Giudizio Finale – Valutazione su Dati Inesplorati

Tutta l'ottimizzazione è stata fatta sul set di validazione. Ora è il momento della verità: come si comportano i nostri sistemi finali (i 4 modelli calibrati e il modello di fusione) sul set di valutazione, dati che non hanno mai visto prima?

| Sistema (sul set di valutazione) | `actDCF` | `minDCF` |
| :------------------------------- | :------- | :------- |
| **Sistema di Fusione**           | **0.1525** | 0.1509   |
| GMM Completa (calibrato)         | 0.1681   | 0.1586   |
| GMM Diagonale (calibrato)        | 0.2216   | 0.1956   |
| SVM (RBF, calibrato)             | 0.3242   | 0.3043   |
| Regolazione Logistica (calibrato)| 0.4609   | 0.4343   |

I risultati sul set di valutazione confermano la gerarchia osservata, con due sorprese:
1.  Il **sistema di fusione è ancora il vincitore assoluto**, con un `actDCF` finale di **0.1525**. Questo dimostra la sua robustezza e capacità di generalizzazione.
2.  Il GMM a covarianza completa, che era secondo sul set di validazione, supera il GMM diagonale sul set di valutazione. Questo tipo di piccole variazioni è normale e sottolinea l'importanza della valutazione finale su dati separati.

Infine, il grafico "Evaluation Results" mostra come le curve del nostro miglior modello (la fusione) si comportino in modo quasi identico tra il set di validazione (linee blu) e quello di valutazione (linee rosse). Questo "gap di generalizzazione" quasi nullo è il sigillo di qualità finale, indicando che non abbiamo subito overfitting e che le performance osservate sono solide e affidabili.

#### Conclusione Definitiva

Questo capitolo finale ha dimostrato il potere della calibrazione e della fusione. La calibrazione si è rivelata essenziale per "recuperare" modelli potenti ma mal calibrati come le SVM. La fusione ha fornito l'ultimo, decisivo passo di ottimizzazione, combinando le forze di tutti i nostri migliori classificatori. Il risultato è un sistema ibrido che non solo è il più performante, ma è anche robusto e ben calibrato, rappresentando il culmine del nostro intero percorso di analisi e modellazione.
