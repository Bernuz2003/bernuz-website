## Argomento 8: Calibrazione e Fusione – L'Atto Finale di Ottimizzazione

Nel nostro percorso attraverso il machine learning, abbiamo costruito un arsenale di classificatori, dai modelli generativi (GMM) ai discriminativi (Regolazione Logistica, SVM). Abbiamo visto che alcuni modelli, come le SVM con kernel RBF e i GMM, offrono un'eccellente capacità di separazione (basso `minDCF`), ma altri, in particolare le SVM, producono punteggi grezzi che sono poco affidabili e "mal calibrati". Questo capitolo finale è dedicato a due tecniche avanzate per affrontare questi problemi:

1. **Calibrazione**: Un processo per trasformare i punteggi grezzi di un classificatore in probabilità a posteriori ben formate, con l'obiettivo di rendere il rischio di Bayes effettivo (`actDCF`) il più vicino possibile al rischio minimo potenziale (`minDCF`).
2. **Fusione**: Una tecnica per combinare le predizioni di più classificatori in un unico "super-modello", sperando di superare le performance di ogni singolo componente.

L'analisi si svolge su una nuova suddivisione dei dati: un set di training (3200 campioni), un set di validazione (1600 campioni) su cui vengono ottimizzate le strategie di calibrazione e fusione, e un set di valutazione finale (1200 campioni), tenuto completamente da parte per la valutazione finale e imparziale del sistema migliore.

### Parte 1: Calibrazione – Insegnare ai Modelli a Parlare il Linguaggio della Probabilità

#### Fondamenti Teorici della Calibrazione

Un classificatore può essere molto bravo a separare le classi (basso `minDCF`) ma pessimo nel quantificare la sua fiducia (alto `actDCF`). Questo fenomeno deriva dal fatto che i punteggi del modello non si comportano effettivamente come **Log-Likelihood Ratios (LLR)** teorici.

Per comprendere il problema, consideriamo il teorema di Bayes nella sua forma logaritmica:

$$\text{LLR}(\mathbf{x}) = \log\frac{P(\mathbf{x}|C_1)P(C_1)}{P(\mathbf{x}|C_0)P(C_0)} = \log\frac{P(\mathbf{x}|C_1)}{P(\mathbf{x}|C_0)} + \log\frac{P(C_1)}{P(C_0)}$$

La soglia ottimale di Bayes per un costo target $\pi_T$ è:

$$\tau = \log\frac{\pi_T}{1-\pi_T}$$

Un modello è **perfettamente calibrato** quando applicare questa soglia teorica ai suoi punteggi produce la performance ottimale. In caso contrario, i punteggi necessitano di calibrazione.

#### La Matematica della Calibrazione: Platt Scaling

La calibrazione viene implementata come una **trasformazione affine** dei punteggi originali, seguita da una sigmoide - una tecnica nota come **Platt Scaling**. In pratica, si addestra un modello di **Regolazione Logistica** utilizzando i punteggi del classificatore originale $s_{\text{orig}}$ come unica feature:

$$s_{\text{calibrato}} = \alpha s_{\text{orig}} + \beta$$

dove $\alpha$ (pendenza) e $\beta$ (intercetta) sono i parametri di calibrazione appresi minimizzando la cross-entropy su un set di calibrazione.

La trasformazione completa include anche la correzione del prior:

$$\text{LLR}_{\text{calibrato}}(\mathbf{x}) = \alpha s_{\text{orig}}(\mathbf{x}) + \beta - \log\frac{p_T}{1-p_T}$$

#### Implementazione della Calibrazione K-Fold

Per evitare overfitting, la calibrazione viene implementata tramite **validazione incrociata K-fold**:

```python
def kfold_calibration_analysis(scores, labels, target_prior=0.1, KFOLD=5):
    training_priors = [0.1, 0.2, 0.3, 0.5, 0.7, 0.8, 0.9]
    results = {}
    
    for pT in training_priors:
        calibrated_scores = []
        calibrated_labels = []
        
        for foldIdx in range(KFOLD):
            SCAL, SVAL = extract_train_val_folds_from_ary(scores, foldIdx, KFOLD)
            LCAL, LVAL_fold = extract_train_val_folds_from_ary(labels, foldIdx, KFOLD)
            
            # Addestra modello di calibrazione (Regressione Logistica)
            w, b = trainWeightedLogRegBinary(vrow(SCAL), LCAL, 0, pT)
            
            # Applica calibrazione con correzione del prior
            calibrated_SVAL = (w.T @ vrow(SVAL) + b - np.log(pT / (1-pT))).ravel()
            
            calibrated_scores.append(calibrated_SVAL)
            calibrated_labels.append(LVAL_fold)
        
        # Valuta performance su scores calibrati
        pooled_scores = np.hstack(calibrated_scores)
        pooled_labels = np.hstack(calibrated_labels)
        
        actDCF = compute_actDCF_binary_fast(pooled_scores, pooled_labels, target_prior, 1.0, 1.0)
        results[pT] = {'actDCF': actDCF, 'calibrated_scores': pooled_scores}
    
    best_pT = min(training_priors, key=lambda p: results[p]['actDCF'])
    return results, best_pT
```

#### Analisi Dettagliata dei Risultati di Calibrazione

I risultati della calibrazione sui nostri quattro modelli migliori rivelano pattern fondamentali che illustrano la natura di ciascun classificatore:

| Modello                 | `actDCF` Originale | `actDCF` Calibrato | Miglioramento | Prior Ottimale |
| :---------------------- | :----------------- | :----------------- | :------------ | :------------- |
| **SVM (RBF)**           | 1.0000             | 0.3111             | **+68.9%**    | $p_T = 0.5$    |
| **Regolazione Logistica** | 0.4823             | 0.4703             | +2.5%         | $p_T = 0.8$    |
| **GMM Diagonale**       | 0.1899             | 0.1873             | +1.4%         | $p_T = 0.9$    |
| **GMM Completa**        | 0.1899             | 0.2060             | -8.5%         | $p_T = 0.2$    |

##### SVM: La Trasformazione Radicale


![](/mlLabs_screens/08_CF/calibration_comparison_svm.png)


La SVM mostra il miglioramento più drammatico, confermando che è un potente separatore ma i suoi punteggi basati sul **margine geometrico** non hanno interpretazione probabilistica intrinseca. La calibrazione è **assolutamente essenziale** per renderla utilizzabile.

**Spiegazione Teorica**: Gli score SVM sono definiti come:
$$f(\mathbf{x}) = \sum_{i} \alpha_i y_i K(\mathbf{x}_i, \mathbf{x}) + b$$

Questi rappresentano la **distanza dal margine**, non log-likelihood ratios. La calibrazione impara la mappatura:
$$P(C_1|\mathbf{x}) \approx \sigma(\alpha f(\mathbf{x}) + \beta)$$

##### GMM: Già Calibrati per Natura

![](/mlLabs_screens/08_CF/calibration_comparison_gmm_full.png)

I modelli GMM, essendo **generativi** e basati su probabilità esplicite, erano già ben calibrati. Il processo di calibrazione ha portato solo a miglioramenti marginali o addirittura peggioramenti per il modello completo.

**Spiegazione Teorica**: I GMM calcolano direttamente le densità:
$$\text{LLR}(\mathbf{x}) = \log p(\mathbf{x}|C_1) - \log p(\mathbf{x}|C_0)$$

Questi score hanno già interpretazione probabilistica naturale, quindi la calibrazione aggiuntiva può introdurre rumore se i dati di calibrazione sono limitati.

##### Regressione Logistica: Fine-Tuning Probabilistico


![](/mlLabs_screens/08_CF/calibration_comparison_logistic_regression.png)


La Regressione Logistica mostra un miglioramento modesto ma consistente. Nonostante sia intrinsecamente probabilistica, la regolarizzazione L2 e l'ottimizzazione discriminativa possono introdurre distorsioni che la calibrazione corregge.

#### Analisi dell'Optimal Training Prior

Un aspetto cruciale è la selezione del **prior di training** $p_T$ per il modello di calibrazione. I risultati mostrano prior ottimali diversi per ciascun modello:

- **SVM**: $p_T = 0.5$ (bilanciato) - il modello originale non ha bias verso alcuna classe
- **Logistic Regression**: $p_T = 0.8$ (sbilanciato verso positivi) - suggerisce un bias originale verso la classe negativa
- **GMM Diagonal**: $p_T = 0.9$ (fortemente sbilanciato) - indica una tendenza conservativa del modello originale
- **GMM Full**: $p_T = 0.2$ (sbilanciato verso negativi) - comportamento opposto al diagonal

### Parte 2: Fusione – L'Unione Fa la Forza

#### Fondamenti Teorici della Fusione

La fusione parte dal principio che **modelli diversi commettono errori diversi**. Se i classificatori hanno **errori non correlati**, la loro combinazione può ridurre la varianza complessiva e migliorare le performance.

Dal punto di vista teorico, consideriamo $M$ classificatori con accuratezze individuali $p_i$ e errori incorrelati. L'accuratezza del sistema di fusione che prende decisioni a maggioranza è superiore a quella di qualsiasi classificatore individuale se più della metà dei classificatori ha accuratezza > 0.5.

#### La Matematica della Fusione: Combinazione Lineare Pesata

La nostra strategia di fusione utilizza la **Regressione Logistica** per apprendere una combinazione lineare ottimale dei punteggi. Trattiamo i punteggi dei quattro sistemi come un vettore di feature a 4 dimensioni:

$$\mathbf{s}_{\text{vettore}} = [s_{\text{GMM-F}}, s_{\text{GMM-D}}, s_{\text{LR}}, s_{\text{SVM}}]^T$$

Il modello di fusione impara:
$$s_{\text{fuso}} = \mathbf{w}^T \mathbf{s}_{\text{vettore}} + b$$

dove i pesi $\mathbf{w}$ sono ottimizzati per minimizzare la cross-entropy:

$$\mathcal{L} = -\frac{1}{N}\sum_{i=1}^N [y_i \log\sigma(s_{\text{fuso}}^{(i)}) + (1-y_i) \log(1-\sigma(s_{\text{fuso}}^{(i)}))]$$

#### Implementazione della Fusione K-Fold

```python
def kfold_fusion_analysis(scores_dict, labels, target_prior=0.1, KFOLD=5):
    score_names = list(scores_dict.keys())
    score_arrays = [scores_dict[name]['scores'] for name in score_names]
    training_priors = [0.1, 0.2, 0.3, 0.5, 0.7, 0.8, 0.9]
    
    results = {}
    
    for pT in training_priors:
        fused_scores = []
        
        for foldIdx in range(KFOLD):
            # Prepara matrici di feature per ciascun fold
            SCAL_arrays = []
            SVAL_arrays = []
            
            for score_array in score_arrays:
                SCAL, SVAL = extract_train_val_folds_from_ary(score_array, foldIdx, KFOLD)
                SCAL_arrays.append(SCAL)
                SVAL_arrays.append(SVAL)
            
            LCAL, LVAL_fold = extract_train_val_folds_from_ary(labels, foldIdx, KFOLD)
            
            # Costruisce matrici di feature
            SCAL_matrix = np.vstack(SCAL_arrays)
            SVAL_matrix = np.vstack(SVAL_arrays)
            
            # Addestra modello di fusione
            w, b = trainWeightedLogRegBinary(SCAL_matrix, LCAL, 0, pT)
            
            # Applica fusione
            fused_SVAL = (w.T @ SVAL_matrix + b - np.log(pT / (1-pT))).ravel()
            fused_scores.append(fused_SVAL)
        
        pooled_scores = np.hstack(fused_scores)
        actDCF = compute_actDCF_binary_fast(pooled_scores, labels, target_prior, 1.0, 1.0)
        results[pT] = {'actDCF': actDCF}
    
    best_pT = min(training_priors, key=lambda p: results[p]['actDCF'])
    return results, best_pT
```

#### Analisi dei Risultati di Fusione


![](/mlLabs_screens/08_CF/fusion_comparison.png)


Dopo aver trovato il prior ottimale ($p_T = 0.1$), il confronto tra sistema di fusione e miglior individuale mostra:

- **Miglior Sistema Individuale** (GMM Diagonale calibrato): `actDCF` = 0.1873
- **Sistema di Fusione**: `actDCF` = **0.1785**
- **Miglioramento**: 4.7%

La fusione ha prodotto un ulteriore miglioramento significativo. Nel mondo dei sistemi ad alte prestazioni, ogni frazione di punto percentuale rappresenta una vittoria importante.

#### Analisi dei Pesi di Fusione

L'analisi dei pesi appresi dal modello di fusione rivela insight interessanti sulla contribuzione di ciascun classificatore:

```python
# Pesi tipici appresi (normalizzati)
fusion_weights = {
    'GMM Full': 0.35,      # Peso elevato - performance buone
    'GMM Diagonal': 0.40,  # Peso massimo - miglior classificatore  
    'Logistic Regression': 0.15,  # Peso moderato - performance limitate
    'SVM': 0.10           # Peso minimo - necessita calibrazione forte
}
```

I pesi riflettono le performance relative: i GMM ricevono i pesi maggiori, la Regressione Logistica un peso moderato, e la SVM il peso minimo nonostante la calibrazione.

### Parte 3: Il Giudizio Finale – Valutazione su Dati Inesplorati

#### Protocollo di Valutazione Rigoroso

Tutta l'ottimizzazione (calibrazione e fusione) è stata svolta sul set di validazione. La valutazione finale avviene su un set **completamente indipendente** che i modelli non hanno mai visto, garantendo una stima non biased delle performance.

#### Risultati Finali sul Set di Valutazione


![](/mlLabs_screens/08_CF/evaluation_results.png)


| Sistema (sul set di valutazione) | `actDCF` | `minDCF` | Calibrazione Gap |
| :------------------------------- | :------- | :------- | :--------------- |
| **Sistema di Fusione**           | **0.1525** | 0.1509   | 0.0016           |
| GMM Completa (calibrato)         | 0.1681   | 0.1586   | 0.0095           |
| GMM Diagonale (calibrato)        | 0.2216   | 0.1956   | 0.0260           |
| SVM (RBF, calibrato)             | 0.3242   | 0.3043   | 0.0199           |
| Regolazione Logistica (calibrato)| 0.4609   | 0.4343   | 0.0266           |

#### Analisi Dettagliata dei Risultati Finali

**Sistema di Fusione - Il Vincitore Assoluto**: 
Con `actDCF` = 0.1525, il sistema di fusione mantiene la sua superiorità anche sui dati di valutazione. Il gap di calibrazione quasi nullo (0.0016) indica un'eccellente qualità probabilistica degli score.

**Inversione di Ranking**: 
Interessante notare come il GMM Full superi il GMM Diagonal sul set di valutazione, invertendo l'ordine osservato in validazione. Questo fenomeno evidenzia l'importanza della valutazione su dati indipendenti e suggerisce che:

1. Il GMM Full ha maggiore capacità di generalizzazione
2. Il GMM Diagonal potrebbe aver subito un leggero overfitting sui dati di validazione
3. La variabilità naturale tra dataset può influenzare il ranking relativo

#### Analisi del Generalization Gap

Il grafico "Evaluation Results" mostra sovrapposizione quasi perfetta tra curve di validazione (blu) e valutazione (rosso) per il sistema di fusione. Questo **gap di generalizzazione** quasi nullo è indicativo di:

- **Robustezza del modello**: Non overfitting sui dati di validazione
- **Stabilità delle performance**: Comportamento consistente su dati diversi  
- **Qualità del processo di ottimizzazione**: La procedura K-fold ha prevenuto l'overfitting

### Analisi Teorica Approfondita

#### Bias-Variance Decomposition nella Fusione

La superiorità del sistema di fusione può essere analizzata attraverso la **decomposizione bias-variance**. Per un ensemble di $M$ classificatori con bias $B_i$ e varianza $V_i$:

$$\text{MSE}_{\text{ensemble}} = \bar{B}^2 + \frac{1}{M}\bar{V} + \frac{M-1}{M}\bar{\rho}\bar{V}$$

dove $\bar{\rho}$ è la correlazione media tra i classificatori. La fusione è efficace quando:
1. $\bar{\rho} < 1$ (errori non completamente correlati)
2. I classificatori hanno bias diversi che si compensano
3. La riduzione della varianza supera l'eventuale aumento del bias

#### Interpretazione Probabilistica della Calibrazione

La calibrazione può essere vista come l'apprendimento della **funzione di calibrazione** $g(\cdot)$ tale che:

$$P(Y=1|g(s)) = g(s)$$

Per un classificatore perfettamente calibrato, la funzione $g$ è l'identità. La deviazione da questa funzione identità quantifica il grado di miscalibrazione del modello originale.

### Conclusioni e Implicazioni Metodologiche

#### Lezioni Apprese dalla Calibrazione

1. **I modelli discriminativi necessitano calibrazione**: SVM e Logistic Regression beneficiano significativamente
2. **I modelli generativi sono naturalmente calibrati**: GMM richiedono calibrazione minima o nulla
3. **Il prior di training influenza l'efficacia**: Ogni modello ha un prior ottimale diverso
4. **La calibrazione può peggiorare modelli già buoni**: GMM Full mostra leggero deterioramento

#### Lezioni Apprese dalla Fusione

1. **La diversità è chiave**: Modelli con approcci diversi (generativo/discriminativo) si complementano bene
2. **La ponderazione automatica è efficace**: I pesi appresi riflettono le performance relative
3. **I miglioramenti sono incrementali ma significativi**: 4.7% di miglioramento in sistemi ad alta performance è sostanziale
4. **La robustezza aumenta**: Il sistema di fusione mostra generalization gap quasi nullo

#### Implicazioni per l'Applicazione Pratica

**Per Sistemi Critici**: La calibrazione è essenziale per fornire stime di confidenza affidabili. La fusione offre robustezza aggiuntiva attraverso la ridondanza.

**Per Deploy in Produzione**: Il sistema di fusione, con il suo gap di calibrazione quasi nullo e la superiorità consistente, rappresenta la scelta ottimale per applicazioni reali.

**Per Ricerca Futura**: I risultati suggeriscono direzioni promettenti:
- Tecniche di calibrazione non-parametrica (isotonic regression)
- Metodi di fusione più sofisticati (stacked generalization)
- Approcci bayesiani alla combinazione di modelli

Questo capitolo finale ha dimostrato il potere trasformativo della calibrazione e della fusione. La calibrazione si è rivelata essenziale per "recuperare" modelli potenti ma mal calibrati come le SVM. La fusione ha fornito l'ultimo, decisivo passo di ottimizzazione, combinando le forze di tutti i nostri migliori classificatori. Il risultato è un sistema ibrido che non solo è il più performante, ma è anche robusto e ben calibrato, rappresentando il culmine dell'intero percorso di analisi e modellazione.
