### Analisi della Riduzione Dimensionale: PCA vs. LDA

L'obiettivo di questo studio è analizzare un dataset di accessi biometrici per valutare la separabilità tra le classi "Genuine" (classe 1) e "Fake" (classe 0). Vengono confrontate due tecniche di riduzione dimensionale, Principal Component Analysis (PCA) e Linear Discriminant Analysis (LDA), sia come strumenti di analisi esplorativa sia come pre-processing per un classificatore.

#### Analisi Esplorativa con Principal Component Analysis (PCA)

La PCA è una tecnica non supervisionata che trasforma i dati in un nuovo sistema di coordinate (le Componenti Principali) per massimizzare la varianza. L'obiettivo è proiettare i dati in uno spazio a dimensionalità inferiore preservando la maggior quantità di informazione possibile.

**Fondamenti Matematici della PCA**
L'obiettivo della PCA può essere formulato come la ricerca di una proiezione che minimizza l'errore quadratico medio di ricostruzione. Dato un set di dati $$x_n$$, se $$\tilde{x}_n$$ è la sua versione ricostruita dopo la proiezione in uno spazio a dimensionalità inferiore, la PCA cerca di minimizzare la funzione di costo:

$$ J = \frac{1}{N} \sum_{n=1}^{N} \|x_n - \tilde{x}_n\|^2 $$

La soluzione a questo problema di minimizzazione si ottiene scegliendo come base per il nuovo spazio gli autovettori della matrice di covarianza dei dati, corrispondenti agli autovalori più elevati. Questi autovettori definiscono le direzioni delle componenti principali.

```python
def compute_PCA(data, m):
    """Calcola la matrice di proiezione PCA con *m* componenti principali."""
    _, cov_matrix = compute_mean_and_covariance(data)
    U, s, _ = np.linalg.svd(cov_matrix)
    
    # Calcola varianza spiegata
    explained_variance_ratio = s / np.sum(s) * 100
    print(f"PCA {m}D: varianza spiegata = {np.sum(explained_variance_ratio[:m]):.1f}%")
    
    return U[:, :m]
```

L'analisi degli istogrammi delle componenti principali mostra come le distribuzioni delle due classi si proiettano su ciascuna delle nuove feature. Si osserva che le prime due componenti, PC1 e PC2, mostrano una certa separazione tra le medie delle classi "Genuine" e "Fake". Le componenti successive (da PC3 a PC6) presentano invece una sovrapposizione molto maggiore.

![](/mlLabs_screens/01_DR/PCA_Full_histograms-1.png)

Questa osservazione è confermata dalla scatter matrix, che visualizza le proiezioni 2D dei dati. La combinazione di PC1 e PC2 offre la separazione visiva più netta tra i punti delle due classi, rafforzando l'idea che l'informazione discriminante sia concentrata nelle prime componenti.

![](/mlLabs_screens/01_DR/PCA_Full_scatter_matrix-1.png)

#### Analisi Discriminante con Linear Discriminant Analysis (LDA)

A differenza della PCA, l'LDA è una tecnica supervisionata che proietta i dati in uno spazio a dimensionalità inferiore con l'obiettivo esplicito di massimizzare la separabilità tra le classi.

**Fondamenti Matematici della LDA**
L'LDA persegue il suo obiettivo massimizzando il rapporto tra la varianza *tra le classi* e la varianza *all'interno delle classi*. Questo si formalizza attraverso due matrici:
1.  La **matrice di scatter between-class** ($$S_b$$), che quantifica la separazione tra le medie delle classi.
2.  La **matrice di scatter within-class** ($$S_w$$), che quantifica la varianza (compattezza) all'interno di ciascuna classe.

L'LDA trova la direzione di proiezione $$\mathbf{w}$$ che massimizza il criterio di Fisher:

$$ J(\mathbf{w}) = \frac{\mathbf{w}^T S_b \mathbf{w}}{\mathbf{w}^T S_w \mathbf{w}} $$

La soluzione $$\mathbf{w}$$ è l'autovettore associato al massimo autovalore del problema agli autovalori generalizzato: $$S_b \mathbf{w} = \lambda S_w \mathbf{w}$$.

```python
def compute_LDA(data, labels):
    """Calcola la direzione discriminante LDA per classificazione binaria."""
    Sb, Sw = compute_scatter_matrices(data, labels)
    eigenvalues, eigenvectors = scipy.linalg.eigh(Sb, Sw)
    
    # Prende l'autovettore con autovalore massimo
    w = eigenvectors[:, -1:]
    
    print(f"LDA: autovalore massimo = {eigenvalues[-1]:.3f}")
    
    return w
```

Applicando l'LDA si ottiene un autovalore massimo di **1.683**. L'istogramma della proiezione LDA mostra che le due classi, proiettate su una singola dimensione, appaiono come due distribuzioni chiaramente separate con una zona di sovrapposizione limitata, indicando l'alta efficacia della tecnica.

![](/mlLabs_screens/01_DR/LDA_Full_histogram-1.png)

### Valutazione delle Performance di Classificazione

Per una valutazione quantitativa, il dataset è stato suddiviso in training set (4000 campioni) e validation set (2000 campioni).

*   **Classificatore LDA Baseline**: Utilizzando l'LDA direttamente sulle 6 feature originali, il modello ha raggiunto un'**accuratezza del 90.7%**.

*   **Classificatore PCA + LDA**: In questa pipeline, la PCA è usata per il pre-processing prima di applicare l'LDA. I risultati al variare del numero di componenti principali sono i seguenti:
    *   PCA (5D) + LDA: **90.7%** di accuratezza
    *   PCA (4D) + LDA: **90.8%** di accuratezza
    *   PCA (3D) + LDA: **90.8%** di accuratezza
    *   PCA (2D) + LDA: **90.8%** di accuratezza
    *   PCA (1D) + LDA: **90.7%** di accuratezza

### Conclusioni

L'analisi dimostra che entrambe le tecniche sono efficaci. Un classificatore LDA sulle feature originali fornisce una baseline robusta (90.7%). Tuttavia, i risultati indicano che è possibile ottenere una performance leggermente superiore (**90.8%**) utilizzando la PCA per ridurre le feature da 6 a 4, 3 o 2 dimensioni prima della classificazione LDA.

Questo suggerisce che la PCA non solo riduce la dimensionalità senza perdita di informazione critica, ma agisce anche come un meccanismo di **denoising**. Eliminando le componenti meno informative e più rumorose, permette all'LDA di operare su uno spazio feature più pulito e compatto. Ottenere un'accuratezza equivalente o superiore con solo 2 feature anziché 6 costituisce un vantaggio significativo in termini di efficienza del modello e robustezza contro l'overfitting.