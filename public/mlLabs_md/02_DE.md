### Argomento 2: Capire i Dati con la Stima di Densità Gaussiana

Dopo aver esplorato i dati con PCA e LDA, facciamo un passo indietro e proviamo a capire la natura fondamentale delle singole feature. Un approccio classico e molto potente è quello di modellare la distribuzione di probabilità di ogni feature. L'ipotesi di partenza, comune in molti campi, è che i dati seguano una **distribuzione Gaussiana (o Normale)**.

L'idea è semplice: per ogni classe ("Genuine" e "Fake") e per ognuna delle 6 feature, cercheremo di trovare la curva a campana (la Gaussiana) che meglio si adatta all'istogramma dei dati osservati. Questo non solo ci darà una comprensione più profonda della struttura dei dati, ma getterà anche le fondamenta per la costruzione di modelli generativi di classificazione.

#### La Matematica dietro il Fitting

Una distribuzione Gaussiana univariata (cioè per una singola variabile) è completamente definita da due parametri: la media $$ \mu $$, che ne determina il centro, e la varianza $$ \sigma^2 $$, che ne definisce la larghezza. La sua funzione di densità di probabilità (PDF) è data dalla celebre formula:

$$
p(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$

Il nostro compito è "imparare" i valori di $$ \mu $$ e $$ \sigma^2 $$ direttamente dai dati. Per farlo, usiamo il principio della **Stima di Massima Verosimiglianza (Maximum Likelihood Estimation - MLE)**. In parole povere, cerchiamo quei valori di $$ \mu $$ e $$ \sigma^2 $$ che rendono massima la probabilità di aver osservato proprio i dati a nostra disposizione.

Per un insieme di dati $$ x_1, x_2, \dots, x_N $$, le stime di massima verosimiglianza per media e varianza sono sorprendentemente intuitive e corrispondono alla media e alla varianza campionaria:

$$
\hat{\mu}_{ML} = \frac{1}{N} \sum_{i=1}^{N} x_i
$$

$$
\hat{\sigma}^2_{ML} = \frac{1}{N} \sum_{i=1}^{N} (x_i - \hat{\mu}_{ML})^2
$$

Nel codice, questo calcolo è implementato in modo molto diretto nella funzione `compute_mu_var_1D`, che sfrutta le funzioni `.mean()` e `.var()` di NumPy, le quali eseguono esattamente queste operazioni.

```python
def compute_mu_var_1D(x):
    """Compute ML estimates for mean and variance of 1D Gaussian."""
    mu = x.mean()
    var = x.var()
    return mu, var
```

Lo script applica questa funzione a ciascuna delle 6 feature, separatamente per i dati appartenenti alla classe 0 (Fake) e alla classe 1 (Genuine), calcolando un totale di 12 coppie di parametri $$(\mu, \sigma^2)$$.

#### Analisi dei Risultati: le Feature sotto la Lente di Ingrandimento

I risultati numerici ottenuti dallo script sono riassunti nella tabella seguente, che mostra le stime di media e varianza per ogni feature e per ogni classe.

| Classe   | Feature | Media Stimata (μ) | Varianza Stimata (σ²) |
| :------- | :------ | :---------------- | :-------------------- |
| **Fake** | 1       | 0.0029            | 0.5696                |
|          | 2       | 0.0187            | 1.4209                |
|          | 3       | -0.6809           | 0.5500                |
|          | 4       | 0.6708            | 0.5360                |
|          | 5       | 0.0280            | 0.6801                |
|          | 6       | -0.0058           | 0.7050                |
| **Genuine**| 1       | 0.0005            | 1.4302                |
|          | 2       | -0.0085           | 0.5783                |
|          | 3       | 0.6652            | 0.5489                |
|          | 4       | -0.6642           | 0.5533                |
|          | 5       | -0.0417           | 1.3178                |
|          | 6       | 0.0239            | 1.2870                |

Dalla tabella emergono subito degli schemi interessanti. Ad esempio, le **feature 3 e 4** sembrano essere particolarmente informative:
*   Per la classe "Fake", la feature 3 ha una media marcatamente negativa ($$-0.68$$) mentre la feature 4 ha una media positiva ($$0.67$$).
*   Per la classe "Genuine", la situazione si inverte specchiularmente: la feature 3 ha una media positiva ($$0.67$$) e la feature 4 ha una media negativa ($$-0.66$$).

Questa inversione di segno delle medie suggerisce che queste due feature, da sole, possiedono un forte potere discriminante. Anche le varianze giocano un ruolo chiave: si noti come per la **feature 2**, la varianza sia molto più alta per la classe "Fake" ($$1.42$$) rispetto a quella "Genuine" ($$0.58$$), indicando una maggiore dispersione dei dati "Fake" lungo questo asse.

L'analisi visiva conferma e arricchisce queste osservazioni. Il grafico generato dallo script mostra gli istogrammi di tutte le feature per entrambe le classi, con sovrapposta la curva Gaussiana stimata.

![](/mlLabs_screens/02_DE/all_features_summary-1.png)

Guardando il grafico, possiamo valutare "a occhio" quanto l'ipotesi Gaussiana sia plausibile.
*   Per le **feature 3 e 4**, le curve a campana si adattano molto bene agli istogrammi, che appaiono simmetrici e ben centrati. Questo rafforza la nostra fiducia nel modello Gaussiano per queste due variabili.
*   Per altre feature, come la **feature 2** della classe "Fake" o la **feature 5** della classe "Genuine", la distribuzione empirica appare più "piatta" e larga (come suggerito dalle loro varianze più elevate) e il fit Gaussiano sembra meno preciso, forse a causa della presenza di code più pesanti o di una leggera asimmetria.

In conclusione, questa analisi univariata ci ha fornito un modello semplice ma efficace per descrivere le distribuzioni di probabilità condizionate al variare della classe, $$ p(x_j | C_k) $$. Abbiamo scoperto che l'assunzione Gaussiana è ragionevole per la maggior parte delle feature e abbiamo identificato quelle più promettenti per la classificazione. Questi parametri stimati sono esattamente ciò di cui avremmo bisogno per costruire un classificatore Naive Bayes Gaussiano, un passo logico successivo nell'analisi di questo dataset.